{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Мне как обычно\", \"Давай как всегда\", \"Да, по дефолту\", \"Не, хочу чего-нибудь новенького\", \"Новеное\", \"Нет, давай новое\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Мне', 'JJ'), ('как', 'NNP'), ('обычно', 'NN')]\n",
      "[('Давай', 'JJ'), ('как', 'NNP'), ('всегда', 'NN')]\n",
      "[('Да', 'NN'), (',', ','), ('по', 'NNP'), ('дефолту', 'NNP')]\n",
      "[('Не', 'NN'), (',', ','), ('хочу', 'JJ'), ('чего-нибудь', 'JJ'), ('новенького', 'NN')]\n",
      "[('Новеное', 'NN')]\n",
      "[('Нет', 'NN'), (',', ','), ('давай', 'NNP'), ('новое', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [\"Чизбургер\", \"Бургер\", \"Чай\", \"Вода\", \"Твистер\", \"Картошка-Фри\", \"Картошку\", \"Кола\", \"Кокакола\", \"Гамбургер\", \"Чипсы\", \"Пирожок\", \"Пончик\", \"Пепси\", \"Баскет\"]\n",
    "def get_dishes(text):\n",
    "    products = []\n",
    "    for product in allowed:\n",
    "        products.append([stemmer.stem(product.lower()), product])\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    res = []\n",
    "    for (word, pos) in tags:\n",
    "        if pos[:2] == 'NN':\n",
    "            for product in products:\n",
    "                if stemmer.stem(word) == product[0]:\n",
    "                    res.append(product[1])\n",
    "    #nouns = [word for (word, pos) in tags if pos[:2] == 'NN']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "diches_texts = [\"Дайте мне колу с бургером\", \"Я хотел бы чизбургер с твистером\", \"Робот, дай ка мне просто чаю\", \"Слыш ведро болтов дай мне пожрать картоху и пирожок\", \"Хочу вкусняшку\", \"Картошку пожалуйста\", \"Дать мне вкусный бургеров\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text: ['Бургер']\n",
      "2 text: ['Твистер']\n",
      "3 text: ['Чай']\n",
      "4 text: ['Пирожок']\n",
      "5 text: []\n",
      "6 text: []\n",
      "7 text: ['Бургер']\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(diches_texts):\n",
    "    print(\"{} text:\".format(i+1), get_dishes(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text: ['Кола', 'Бургер']\n",
      "2 text: ['Чизбургер', 'Твистер']\n",
      "3 text: ['Чай']\n",
      "4 text: ['Пирожок']\n",
      "5 text: []\n",
      "6 text: ['Картошку']\n",
      "7 text: ['Бургер']\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(diches_texts):\n",
    "    print(\"{} text:\".format(i+1), get_dishes(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_or_no(text):\n",
    "    yes = [\"да\", \"обычно\", \"дефолт\", \"всегда\", \"наверно\"]\n",
    "    yes_nlp_optimized = [stemmer.stem(i) for i in yes]\n",
    "    no = [\"нет\", \"новенькое\", \"не\", \"интересное\", \"необычное\", \"нестандартное\"]\n",
    "    no_nlp_optimized = [stemmer.stem(i) for i in no]\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    for i in tokens:\n",
    "        i = stemmer.stem(i)\n",
    "        if i in yes_nlp_optimized:\n",
    "            return True\n",
    "        if i in no_nlp_optimized:\n",
    "            return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text: True\n",
      "2 text: True\n",
      "3 text: True\n",
      "4 text: False\n",
      "5 text: False\n",
      "6 text: False\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    print(\"{} text:\".format(i+1), get_yes_or_no(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"картошка\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
